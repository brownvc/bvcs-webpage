<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

	<title>Brown Visual Computing Seminar</title>

	<link rel="stylesheet" href="css/fonts.css" type="text/css" charset="utf-8">
	<link rel="stylesheet" href="css/normalize.css" type="text/css" charset="utf-8">
	<link rel="stylesheet" href="css/main.css" type="text/css" charset="utf-8">
	<link rel="stylesheet" href="css/jht.css" type="text/css" charset="utf-8">

	<script type="text/javascript">		
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-23628422-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>
    
    <link rel="icon" type="image/png" href="logos/BVC_logo.png">
    <title>Brown Visual Computing Seminar</title>
    <meta property='og:title' content='Brown Visual Computing Seminar'/>
    <meta property='og:url' content='https://visual.cs.brown.edu/bvcs' />
    <meta property='og:image' content='https://visual.cs.brown.edu/bvcs/poster.jpg' />
    <meta property="og:type" content="website" />
</head>

<body style="max-width: none;">
	<div class="right-column" style="max-width: 45em; margin: auto;; margin-top: 3em">

		<h1><b><a href="https://visual.cs.brown.edu/" target="_blank">Brown Visual Computing</a> Seminar</b></h1>
		
        <p class="phototext" style="max-width: 45em;">
        Fridays, 12 noon ET<br/>
		<!-- Add to calendar-->
		</p>
		
		<p>
			<a href="Final_V1.jpeg" target="_blank"><img src="Final_V1.jpeg" class="teaser-image" width="640px"></a>
		</p>

		<p>The Brown Visual Computing Seminar is a series of talks organized by the <a href="https://visual.cs.brown.edu/" target="_blank">Visual Computing Group</a> at Brown University. The seminar will have talks by experts on topics such as computer vision, computer graphics, HCI, animation, visualization, artificial intelligence, and machine learning. It will be held on select Fridays each semester at 12 noon Eastern Time.</p>
			<p>Most talks will be made available live via Zoom Webinar, and recorded via YouTube. Please click on the link next to the speaker to add to your calendar.
		</p>
		<h2>Fall 2022 Speakers</h2>
		<p>All talks are at 12 noon Eastern Time.</p>
		<div class="container">
			<table id="speakers_table">				
				<tbody>
					<tr>
						<th>Date</th>
						<th>Speaker</th>
						<th>Affiliation</th>
						<th>Title</th>
						<th>Location</th>
						<th>Calendar</th>
					</tr>					
					<tr>
						<td>30-Sep-2022</td>
						<td><a href="https://zollhoefer.com/" target="_blank">Michael Zollhoefer</a></td>
						<td>Meta Reality Labs Research (RLR)</td>
						<td>Complete Codec Telepresence
							<div onclick="toggleDetails(1)" id="moreDetailsBtn_1" class="showBtn">
								<a id="showDetails_1">+ Show details</a>
								<a id="hideDetails_1" style="display:none;">- Hide details</a>								
							</div>
						</td>
						<td><a href="" target="_blank">Recording Coming Soon</a></td>
						<td></td>
					</tr>	
					<tr id="rowDetails_1" style="display:none;">
						<td colspan="6">
							<p style="font-size: 20px;">Complete Codec Telepresence</p>
							<p>
							Imagine two people, each of them within their own home, being able to communicate and interact virtually with each other as if they are both present in the same shared physical space. Enabling such an experience, i.e., building a telepresence system that is indistinguishable from reality, is one of the goals of Reality Labs Research (RLR) in Pittsburgh. To this end, we develop key technology that combines fundamental computer vision, machine learning, and graphics techniques based on a novel neural reconstruction and rendering paradigm. In this talk, I will cover our advances towards a neural rendering approach for complete codec telepresence that includes metric avatars, binaural audio, photorealistic spaces, as well as their interactions in terms of light and sound transport. In the future, this approach will bring the world closer together by enabling anybody to communicate and interact with anyone, anywhere, at any time, as if everyone were sharing the same physical space.
							</p>
							
							<p style="font-size: 20px;">Bio</p>
							<p>Michael Zollhoefer is a Research Scientist at Meta Reality Labs Research (RLR) in Pittsburgh leading the Completeness Group. His north star is fully immersive remote communication and interaction in the virtual world at a level that is indistinguishable from reality. To this end, he develops key technology that combines fundamental computer vision, machine learning, and graphics research based on a novel neural reconstruction and rendering paradigm. Before joining RLR, Michael was a Visiting Assistant Professor at Stanford University and a Postdoctoral Researcher at the Max Planck Institute for Informatics. He received his PhD from the University of Erlangen-Nuremberg for his work on real-time reconstruction of static and dynamic scenes.</p>
						</td>
					</tr>					
					<tr>
						<td>7-Oct-2022</td>
						<td><a href="https://www.landay.org/" target="_blank">James Landay</a></td>
						<td>Stanford University</td>
						<td>“AI For Good” Isn’t Good Enough: A Call for Human-Centered AI
							<div onclick="toggleDetails(2)" id="moreDetailsBtn_2" class="showBtn">
								<a id="showDetails_2">+ Show details</a>
								<a id="hideDetails_2" style="display:none;">- Hide details</a>								
							</div>
						</td>
						<td><a href="https://brown.zoom.us/j/97233372116" target="_blank">Zoom</a></td>
						<td><a href="https://calendar.google.com/event?action=TEMPLATE&tmeid=MGQ2Nm5saTA2aWF0YjgzdDkxb3BhYzMzMWsgY181ZDZkOWJmZWMyOGZlYTgzZDJlMmNkODQ2MTY5MmRjOTY4ZDAxNTNlN2QxNjQ2MGRiNTlhNjRhNDg3NWM0ZTk5QGc&tmsrc=c_5d6d9bfec28fea83d2e2cd8461692dc968d0153e7d16460db59a64a4875c4e99%40group.calendar.google.com" target="_blank">Add to calendar</a></td>
					</tr>	
					<tr id="rowDetails_2" style="display:none;">
						<td colspan="6">
							<p style="font-size: 20px;">“AI For Good” Isn’t Good Enough: A Call for Human-Centered AI</p>
							<p>
								The growing awareness of the pervasiveness of AI’s impact on humans and societies has led to a proliferation of “AI for Good” initiatives. I argue that simply recognizing the potential impacts of AI systems is only table stakes for developing and guiding societally positive AI. Blindly applying AI techniques to a problem in an important societal area, such as healthcare, often leads to solving the wrong problem. In this talk, I will advance the idea that to be truly Human-Centered, the development of AI must change in three ways: it must be user-centered, community-centered, and societally-centered. First, user-centered design integrates well-known techniques to account for the needs and abilities of a system’s end users while rapidly improving a design through rigorous iterative user testing. Combined with creative new ideas and technologies, user-centered design helps move from designing systems that try to replicate humans to AI systems that work for humans. Second, AI systems also have impacts on communities beyond the direct users—Human-Centered AI must be community-centered and engage communities, e.g., with participatory techniques, at the earliest stages of design. Third, these impacts can reverberate at a societal level, requiring forecasting and mediating potential impacts throughout a project as well. To accomplish these three changes, successful Human-Centered AI requires the early engagement of multidisciplinary teams beyond technologists, including experts in design, the social sciences and humanities, and domains of interest such as medicine or law. In this talk I will elaborate on my argument for an authentic Human-Centered AI by showing both negative and positive examples. I will also illustrate how my own group’s research in health, wellness, and behavior change is both living up to and failing in meeting the needs of a Human-Centered AI design process.
							</p>
							
							<p style="font-size: 20px;">Bio</p>
							<p>
								James Landay is a Professor of Computer Science and the Anand Rajaraman and Venky Harinarayan Professor in the School of Engineering at Stanford University. He specializes in human-computer interaction. Landay is the co-founder and Associate Director of the Stanford Institute for Human-centered Artificial Intelligence (HAI). Prior to joining Stanford, Landay was a Professor of Information Science at Cornell Tech in New York City for one year and a Professor of Computer Science & Engineering at the University of Washington for 10 years. From 2003-2006, he also served as the Director of Intel Labs Seattle, a leading research lab that explored various aspects of ubiquitous computing. Landay was also the chief scientist and co-founder of NetRaker, which was acquired by KeyNote Systems in 2004. Before that he was an Associate Professor of Computer Science at UC Berkeley. Landay received his BS in EECS from UC Berkeley in 1990, and MS and PhD in Computer Science from Carnegie Mellon University in 1993 and 1996, respectively. His PhD dissertation was the first to demonstrate the use of sketching in user interface design tools. He is a member of the ACM SIGCHI Academy and an ACM Fellow. He served for six years on the NSF CISE Advisory Committee.
							</p>
						</td>
					</tr>
					<tr>
						<td>14-Oct-2022</td>
						<td><a href="https://www.biostat.wisc.edu/~yli/" target="_blank">Yin Li</a></td>
						<td>UW Madison</td>
						<td>Learning Visual Knowledge from Paired Image-Text Data
							<div onclick="toggleDetails(3)" id="moreDetailsBtn_3" class="showBtn">
								<a id="showDetails_3">+ Show details</a>
								<a id="hideDetails_3" style="display:none;">- Hide details</a>								
							</div>
						</td>
						<td><a href="https://brown.zoom.us/j/97233372116" target="_blank">Zoom</a></td>
						<td><a href="https://calendar.google.com/event?action=TEMPLATE&tmeid=M2ltaGhlbGtkYWtsa2V1bmE1cjVuaWFtMWkgY181ZDZkOWJmZWMyOGZlYTgzZDJlMmNkODQ2MTY5MmRjOTY4ZDAxNTNlN2QxNjQ2MGRiNTlhNjRhNDg3NWM0ZTk5QGc&tmsrc=c_5d6d9bfec28fea83d2e2cd8461692dc968d0153e7d16460db59a64a4875c4e99%40group.calendar.google.com" target="_blank">Add to calendar</a></td>
					</tr>	
					<tr id="rowDetails_3" style="display:none;">
						<td colspan="6">
							<p style="font-size: 20px;">Learning Visual Knowledge from Paired Image-Text Data</p>
							<p>
								Images and their text descriptions (e.g., captions) are readily available in great abundance over the Internet, creating a unique opportunity and a recent surge of interest to develop deep models for image understanding. An image contains millions of pixels capturing the intensity and color of a visual scene. Yet the same scene can be oftentimes summarized using dozens of words. How can we bridge the gap between visual and text data? What can we learn from these image-text pairs? In this talk, I will describe our recent work to address these research questions, with a focus on learning visual knowledge from images and their captions.

								First, I will talk about our work on vision-language representation learning for matching images and sentences, and for aligning image regions with text tokens. Our latest development demonstrates that region representations can be learned from images and their captions, and enable zero-shot and open-vocabulary object detection. Moving forward, I will present our work on learning from image-text pairs to detect image scene graphs -- a graphical representation that captures localized visual concepts (e.g., object names) and their relationships (e.g., predicates). I will further describe how these scene graphs can be used to reason about different scene components within an image. Lastly, I will discuss the limitations of existing vision-language models learned by passively observing image-text data, and briefly introduce our ongoing effort of active learning from first person visual experience.
							</p>
							
							<p style="font-size: 20px;">Bio</p>
							<p>
								Yin Li is an Assistant Professor in the Department of Biostatistics and Medical Informatics and affiliate faculty in the Department of Computer Sciences at the University of Wisconsin-Madison. Previously, he obtained his PhD in computer science from Georgia Tech and was a postdoctoral fellow in the Robotics Institute at Carnegie Mellon University. His primary research focus is computer vision. He is also interested in the applications of vision and learning in healthcare. He has been serving as area chairs for the top vision and AI conferences, including CVPR, ICCV, ECCV and IJCAI. He was the co-recipient of the best student paper awards at MobiHealth 2014 and IEEE Face and Gesture 2015, and the best demo nominee at ECCV 2020. His work was covered by MIT Tech Review, WIRED UK, New Scientist, BBC, and Forbes.
							</p>
						</td>
					</tr>
					<tr>
						<td>11-Nov-2022</td>
						<td><a href="https://www.dgp.toronto.edu/~hsuehtil/" target="_blank">Derek Liu</a></td>
						<td>U Toronto / Roblox Research</td>
						<td>Generative Models for Stylized Geometry
							<div onclick="toggleDetails(4)" id="moreDetailsBtn_4" class="showBtn">
								<a id="showDetails_4">+ Show details</a>
								<a id="hideDetails_4" style="display:none;">- Hide details</a>								
							</div>
						</td>
						<td><a href="https://brown.zoom.us/j/95422192763" target="_blank">Zoom</a></td>
						<td><a href="https://calendar.google.com/event?action=TEMPLATE&tmeid=M2ltaGhlbGtkYWtsa2V1bmE1cjVuaWFtMWkgY181ZDZkOWJmZWMyOGZlYTgzZDJlMmNkODQ2MTY5MmRjOTY4ZDAxNTNlN2QxNjQ2MGRiNTlhNjRhNDg3NWM0ZTk5QGc&tmsrc=c_5d6d9bfec28fea83d2e2cd8461692dc968d0153e7d16460db59a64a4875c4e99%40group.calendar.google.com" target="_blank">Add to calendar</a></td>
					</tr>	
					<tr id="rowDetails_4" style="display:none;">
						<td colspan="6">
							<p style="font-size: 20px;">Generative Models for Stylized Geometry</p>
							<p>
								Recent advances in stylizing 2D digital content have sparked a plethora of image stylization and non-photorealistic rendering technologies. However, how to generate stylized 3D geometry remains a challenging problem. One major reason is the lack of suitable "languages" for computers to understand the style of 3D objects. In this talk, I will cover three perspectives for capturing geometric styles, including rendering, machine-learned geometric prior, and surface normals. I will demonstrate how these perspectives can enable computers to generate stylized 3D content. I argue that exploring fundamental style elements for geometry would unlock the door to learning-based and optimization-based techniques for geometric stylization.
							</p>
							
							<p style="font-size: 20px;">Bio</p>
							<p>
								Hsueh-Ti Derek Liu is a Research Scientist at Roblox working on digital geometry processing and 3D machine learning. Derek's work focuses on developing easy-to-use 3D modeling tools and numerical methods for processing geometric data at scale. He obtained his PhD at the University of Toronto advised by Prof. Alec Jacobson. He worked as a visiting scholar at École Polytechnique in 2019, working with Prof. Maks Ovsjanikov. He completed his M.S. with Profs. Keenan Crane and Levent Burak Kara at Carnegie Mellon University.
							</p>
						</td>
					</tr>
					<tr>
						<td>18-Nov-2022</td>
						<td><a href="" target="_blank">Mackenzie Leake</a></td>
						<td>MIT</td>
						<td>TBA
							<div onclick="toggleDetails(5)" id="moreDetailsBtn_5" class="showBtn">
								<a id="showDetails_5">+ Show details</a>
								<a id="hideDetails_5" style="display:none;">- Hide details</a>								
							</div>
						</td>
						<td><a href="" target="_blank">Zoom</a></td>
						<td><a href="" target="_blank">Add to calendar</a></td>
					</tr>	
					<tr id="rowDetails_5" style="display:none;">
						<td colspan="6">
							<p style="font-size: 20px;">TITLE</p>
							<p>
								TBA
							</p>
							
							<p style="font-size: 20px;">Bio</p>
							<p>
								TBA
							</p>
						</td>
					</tr>
					<tr>
						<td>2-Dec-2022</td>
						<td><a href="" target="_blank">Andrea Tagliasacchi</a></td>
						<td>Simon Fraser / Google</td>
						<td>TBA
							<div onclick="toggleDetails(6)" id="moreDetailsBtn_6" class="showBtn">
								<a id="showDetails_6">+ Show details</a>
								<a id="hideDetails_6" style="display:none;">- Hide details</a>								
							</div>
						</td>
						<td><a href="" target="_blank">Zoom</a></td>
						<td><a href="" target="_blank">Add to calendar</a></td>
					</tr>	
					<tr id="rowDetails_6" style="display:none;">
						<td colspan="6">
							<p style="font-size: 20px;">TITLE</p>
							<p>
								TBA
							</p>
							
							<p style="font-size: 20px;">Bio</p>
							<p>
								TBA
							</p>
						</td>
					</tr>
					<tr>
						<td>9-Dec-2022</td>
						<td><a href="" target="_blank">Pat Hanrahan</a></td>
						<td>Stanford</td>
						<td>TBA
							<div onclick="toggleDetails(7)" id="moreDetailsBtn_7" class="showBtn">
								<a id="showDetails_7">+ Show details</a>
								<a id="hideDetails_7" style="display:none;">- Hide details</a>								
							</div>
						</td>
						<td><a href="" target="_blank">Zoom</a></td>
						<td><a href="" target="_blank">Add to calendar</a></td>
					</tr>	
					<tr id="rowDetails_7" style="display:none;">
						<td colspan="6">
							<p style="font-size: 20px;">TITLE</p>
							<p>
								TBA
							</p>
							
							<p style="font-size: 20px;">Bio</p>
							<p>
								TBA
							</p>
						</td>
					</tr>
				</tbody>
			</table>
		</div>		

		<h2>Acknowledgements</h2>
		
		<p>
			Thanks to <a href="https://cs.brown.edu/~sk/" target="_blank">Shriram Krishnamurthi</a> and <a href="https://www.tarunrajnish.com" target="_blank">Tarun Rajnish</a>.
		</p>
		
		<table style="min-width: 625px; margin: auto; text-align: center;">
			<tbody>
				<tr>
					<td>
						<a href="https://cs.brown.edu" target="_blank"><img src="logos/BrownCSLogo.png" height=70px></a>
                    </td>
					<td>
						<a href="https://visual.cs.brown.edu" target="_blank"><img src="logos/BVC_logo.png" height=70px></a>
                    </td>
				</tr>
			</tbody>
		</table>
		
		<!-- Little bit of space -->
		<p></p>
		
		<script>
			function toggleDetails(rowNumber) {
				var rowDetails = document.getElementById("rowDetails_" + rowNumber);
				var showDetails = document.getElementById("showDetails_" + rowNumber);
				var hideDetails = document.getElementById("hideDetails_" + rowNumber);
				if (showDetails.style.display !== "none") {
				rowDetails.style.display = "contents";
				showDetails.style.display = "none";
				hideDetails.style.display = "inline";
				} else {
				rowDetails.style.display = "none";
				showDetails.style.display = "inline";
				hideDetails.style.display = "none";
				}
			}
		</script>
		
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-SSQESWNE32"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-SSQESWNE32');
		</script>
	</div>
</body>
</html>
